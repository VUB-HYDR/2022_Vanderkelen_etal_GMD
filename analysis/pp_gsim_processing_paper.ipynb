{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing for mizuRoute global Hanasaki simulations\n",
    "## Evaluation with GSIM indices\n",
    "\n",
    "postprocessing from Cheyenne - global simulations with HDMA topology\n",
    "\n",
    "Inne Vanderkelen - March 2021\n",
    "\n",
    "**TO DO:**\n",
    "* Manually check offset location GSIM stations\n",
    "* double check joined gdf is correct? \n",
    "* fix KGE metric (problem with correlation calculation of pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import hydroeval as he\n",
    "import pickle\n",
    "import utils\n",
    "import warnings\n",
    "import cartopy as cart\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plot settings\n",
    "utils.set_plot_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings\n",
    "intersect_stations = False\n",
    "classify_controlled = False # for all station, classify controlled stations based on downstream distance from reservoir. If flase, load values\n",
    "plot = False\n",
    "calc_dict=True\n",
    "# only use stations defined as controlled\n",
    "flag_only_controlled = True\n",
    "calc_stations = False\n",
    "sort_indices = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialisation\n",
    "\n",
    "# model directory\n",
    "outdir = '/glade/work/ivanderk/mizuRoute_global/route/'\n",
    "\n",
    "# current working directory\n",
    "scriptsdir = os.getcwd() + '/'\n",
    "\n",
    "# observations dir\n",
    "datadir = '/glade/work/ivanderk/data/'\n",
    "\n",
    "# give simulation name\n",
    "casename= 'natlake'\n",
    "case_H06 =  'H06'\n",
    "case_nolake =  'nolake'\n",
    "case_natlake =  'natlake'\n",
    "\n",
    "cases = [case_nolake,case_natlake,case_H06]\n",
    "cases_longname = [  'no lakes','natural lakes','Hanasaki']\n",
    "colors = [ 'skyblue','yellowgreen', 'coral']\n",
    "\n",
    "# simulation years\n",
    "# simulation years\n",
    "nspinup = 2 # number of spin up years \n",
    "start_year = 1979 + nspinup\n",
    "end_year = 2000\n",
    "\n",
    "timestep = 'monthly' # monthly # yearly\n",
    "\n",
    "units = {'KGE': '-', 'NSE': '-', 'RMSE': 'mÂ³/s', 'MARE': '-','PBIAS': '%'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "\n",
    "# open river topology as dataset (immediately correct spaces before PFAF code)\n",
    "ntopo = xr.open_dataset(outdir+'ancillary_data/ntopo_hdma_old_mod.reorder_lake_H06.nc')\n",
    "#ntopo = xr.open_dataset(outdir+'ancillary_data/ntopo_hdma_mod.reorder_lake_H06.nc')\n",
    "df_metadata = pd.read_csv(datadir+'gsim/gsim_metadata/GSIM_catalog/GSIM_metadata.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare GSIM obs for model evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get GSIM stations that are on river network\n",
    "if flag intersect_stations is true: do intersection and save files \n",
    "if not, just load csv with list of intersecting stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "# relate GSIM stations to river segments based on offset \n",
    "offset = 0.002 # degrees (~200 m on equator)\n",
    "\n",
    "# threshold reservoir influence\n",
    "threshold_res_influence = 200e3 # m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.3 ms, sys: 16.4 ms, total: 93.7 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if intersect_stations: # calculate intersection, otherwise load\n",
    "\n",
    "    # open gsim meta data, correct for suspect coordinates, and convert into geodataframe\n",
    "    df_suspect_coordinates =  pd.read_csv(datadir+'gsim/gsim_metadata/GSIM_catalog/GSIM_suspect_coordinates_stations.csv')\n",
    "    df_metadata = df_metadata[~df_metadata['gsim.no'].isin(df_suspect_coordinates['gsim.no'])]\n",
    "    gdf_stations_raw = gpd.GeoDataFrame(df_metadata, geometry=gpd.points_from_xy(df_metadata.longitude, df_metadata.latitude))\n",
    "\n",
    "    # open river topology as geodataframe (necessary for intersection)\n",
    "    gdf_river = gpd.read_file(datadir+'topology/river_with_lake_flag4_reorder/river_with_lake_flag4_reorder.shp')\n",
    "    #gdf_river = gpd.read_file(datadir+'topology/HDMA_hydrolakes10km_reorder/river_with_lake_flag4_10km_reorder.shp')\n",
    "    \n",
    "    stations_buffer = gdf_stations_raw\n",
    "    stations_buffer['geometry'] = stations_buffer.buffer(offset)\n",
    "    joined = gpd.sjoin(stations_buffer,gdf_river, how='inner', op='intersects')\n",
    "    gdf_stations_intersect = gdf_stations_raw.loc[gdf_stations_raw['gsim.no'].isin(joined['gsim.no'])]\n",
    "    print(str(gdf_stations_intersect.shape[0])+' of '+str(gdf_stations_raw.shape[0])+' stations on river network ('+str(round(gdf_stations_intersect.shape[0]/gdf_stations_raw.shape[0]*100,1))+'%)')\n",
    "\n",
    "    # save merged info in CSV\n",
    "    joined.drop(['geometry'], axis='columns', inplace=True)\n",
    "    joined.to_csv(datadir+'gsim/stations/stations_HDMA_intersect.csv')\n",
    "    \n",
    "    \n",
    "    # save intersecting stations in shapefile\n",
    "    gdf_stations_intersect.to_file(datadir+'gsim/stations/stations_HDMA_intersect.shp')\n",
    "    \n",
    "\n",
    "    stations = joined\n",
    "    \n",
    "else: \n",
    "    \n",
    "    stations = pd.read_csv(datadir+'gsim/stations/stations_HDMA_intersect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classify GSIM stations as reservoir controlled and natural flow\n",
    "by adding extra column to stations metadata 'controlled' with 1 if yes, and 0 if no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify GSIM stations as controlled river stream and naturalised\n",
    "if classify_controlled: \n",
    "\n",
    "    # get pfafs corresponding to segments to hrus trough hru_seg_id\n",
    "    seg_pfafs = ntopo[['PFAF','seg_id','lake_model']].to_dataframe()\n",
    "    # correct for spaces in front of the PFAF code\n",
    "    seg_pfafs['PFAF'] = np.char.strip( seg_pfafs['PFAF'].values.astype(str))\n",
    "    res_pfaf = seg_pfafs.loc[seg_pfafs['lake_model']==2,'PFAF']\n",
    "\n",
    "    res_is_station_pfaf = res_pfaf[res_pfaf.isin(stations['PFAF'])]\n",
    "\n",
    "    # for every reservoir pfaf get downstream pfafs along threshold\n",
    "    resstream_pfaf = utils.get_rescontrolled_pfafs(ntopo,res_pfaf.values,threshold_res_influence)\n",
    "\n",
    "    # assign controlled stations\n",
    "    stations.loc[stations['PFAF'].isin(resstream_pfaf), 'controlled'] = 1\n",
    "\n",
    "    # save as csv\n",
    "    stations.to_csv(datadir+'gsim/stations/stations_HDMA_controlled.csv')\n",
    "\n",
    "    # save as shpfile \n",
    "    gdf_stations_intersect = gpd.read_file(datadir+'gsim/stations/stations_HDMA_intersect.shp')\n",
    "    gdf_stations_intersect.merge(stations,on='gsim.no').to_file(datadir+'gsim/stations/stations_HDMA_intersect_controlled.shp')\n",
    "    stations_controlled = stations.loc[stations.controlled==1]\n",
    "\n",
    "    \n",
    "else: \n",
    "    stations = pd.read_csv(datadir+'gsim/stations/stations_HDMA_controlled.csv')\n",
    "    stations_controlled = stations.loc[stations.controlled==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare GSIM with simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get GSIM stations that include modeled time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations in modeled period: 10233\n"
     ]
    }
   ],
   "source": [
    "start_year_obs = pd.to_datetime(stations['year.start'], format='%Y')\n",
    "end_year_obs = pd.to_datetime(stations['year.end'], format='%Y')\n",
    "latest_start = stations['year.start'].apply(lambda x: max(x,start_year))\n",
    "earliest_end = stations['year.end'].apply(lambda x: min(x,end_year))\n",
    "\n",
    "stations = stations[(earliest_end-latest_start) >0]\n",
    "print('Number of stations in modeled period: '+str(stations.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load mizuRoute simulation masked for GSIM stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nolake\n",
      "Loading year 1981\n",
      "Loading year 1982\n",
      "Loading year 1983\n",
      "Loading year 1984\n",
      "Loading year 1985\n",
      "Loading year 1986\n",
      "Loading year 1987\n",
      "Loading year 1988\n",
      "Loading year 1989\n",
      "Loading year 1990\n",
      "Loading year 1991\n",
      "Loading year 1992\n",
      "Loading year 1993\n",
      "Loading year 1994\n",
      "Loading year 1995\n",
      "Loading year 1996\n",
      "Loading year 1997\n",
      "Loading year 1998\n",
      "Loading year 1999\n",
      "Loading year 2000\n",
      "natlake\n",
      "Loading year 1981\n",
      "Loading year 1982\n",
      "Loading year 1983\n",
      "Loading year 1984\n",
      "Loading year 1985\n",
      "Loading year 1986\n",
      "Loading year 1987\n",
      "Loading year 1988\n",
      "Loading year 1989\n",
      "Loading year 1990\n",
      "Loading year 1991\n",
      "Loading year 1992\n",
      "Loading year 1993\n",
      "Loading year 1994\n",
      "Loading year 1995\n",
      "Loading year 1996\n",
      "Loading year 1997\n",
      "Loading year 1998\n",
      "Loading year 1999\n",
      "Loading year 2000\n",
      "H06\n",
      "Loading year 1981\n",
      "Loading year 1982\n",
      "Loading year 1983\n",
      "Loading year 1984\n",
      "Loading year 1985\n",
      "Loading year 1986\n",
      "Loading year 1987\n",
      "Loading year 1988\n",
      "Loading year 1989\n",
      "Loading year 1990\n",
      "Loading year 1991\n",
      "Loading year 1992\n",
      "Loading year 1993\n",
      "Loading year 1994\n",
      "Loading year 1995\n",
      "Loading year 1996\n",
      "Loading year 1997\n",
      "Loading year 1998\n",
      "Loading year 1999\n",
      "Loading year 2000\n",
      "CPU times: user 19.3 s, sys: 40.7 s, total: 1min\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if calc_dict:\n",
    "    \n",
    "    # for different cases \n",
    "    # create mask with segids for grdc stations that are classified as controlled\n",
    "    mask_stations = ntopo['seg_id'].astype('int64').isin(list(stations_controlled['seg_id'].values)) \n",
    "\n",
    "    #initialise empty dictionary for storing mizuroute simulations\n",
    "    ds_sim_dict = {}\n",
    "\n",
    "    for case in cases: \n",
    "        print(case)\n",
    "        # laod all years and append into one dataset with outflow\n",
    "        ds_res_year_all =  []\n",
    "        for year in range(start_year,end_year+1):\n",
    "            print('Loading year '+str(year)+'\\r')\n",
    "            # Open simulation for one year\n",
    "            ds = xr.open_dataset(outdir+\"output/\"+case+\".mizuRoute.h.\"+str(year)+\"-01-01-00000.nc\")\n",
    "\n",
    "            # extract reservoir outflow \n",
    "            ds_res_year = ds['IRFroutedRunoff'].where(mask_stations).to_dataset(name='discharge')\n",
    "\n",
    "            ds_res_year_all.append(ds_res_year)\n",
    "\n",
    "        ds_mod = xr.concat(ds_res_year_all, dim='time')\n",
    "        ds_sim_dict[case] = ds_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load GSIM observations per station and compare with mizuRoute simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_dict=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 33s, sys: 720 ms, total: 22min 34s\n",
      "Wall time: 22min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if calc_dict: \n",
    "    obs_dict = {}\n",
    "    mod_dict = {}\n",
    "    metric_dict = {}\n",
    "\n",
    "    for i, gsim_no in enumerate(stations_controlled['gsim.no']): \n",
    "        #i = 1\n",
    "\n",
    "        print('Processing station '+str(i)+ ' of '+str(len(stations_controlled['gsim.no'])),end='\\r' )\n",
    "\n",
    "\n",
    "        if timestep=='monthly': \n",
    "            fn = datadir+'gsim/gsim_indices/TIMESERIES/monthly/'+gsim_no+'.mon'\n",
    "            freq='M'\n",
    "        elif timestep == 'yearly': \n",
    "            fn = datadir+'gsim/gsim_indices/TIMESERIES/yearly/'+gsim_no+'.year'\n",
    "            freq='Y'\n",
    "        elif timestep == 'seasonal': \n",
    "            fn = datadir+'gsim/gsim_indices/TIMESERIES/seasonal/'+gsim_no+'.seas'\n",
    "\n",
    "        if not os.path.isfile(fn):\n",
    "            print('No '+timestep+' observations for station '+str(gsim_no))\n",
    "        else:\n",
    "            try:\n",
    "                df_obs = pd.read_csv(fn,header=21, delimiter=',\\t')\n",
    "                df_obs.rename(columns={'\"date\"':'date','\"MEAN\"':'MEAN','\"SD\"':'SD','\"CV\"':'CV','\"IQR\"':'IQR','\"MIN\"':'MIN','\"MAX\"':'MAX','\"MIN7\"':'MIN7','\"MAX7\"':'MAX7','\"n.missing\"':'n.missing','\"n.available\"':'n.available'}, inplace=True)\n",
    "                df_obs['date'] = pd.to_datetime(df_obs['date']).apply(lambda x: x if x.month != 2 and x.date != 29 else pd.datetime(x.year, x.month, 28)) # convert to datetime and replace all leap days (mizuroute does not have leap days)\n",
    "                df_obs['date'] = df_obs['date']\n",
    "                df_obs = df_obs.set_index('date')\n",
    "            except: \n",
    "                print('Error loading '+timestep+' obs for station '+str(gsim_no))\n",
    "\n",
    "            # get obs and mod overlapping timeseries\n",
    "            sim_period = ds_sim_dict[cases[0]].indexes['time'].to_datetimeindex()\n",
    "            period_overlap = sim_period.intersection(df_obs.index)\n",
    "            df_obs = df_obs.loc[df_obs.index.isin(period_overlap)]\n",
    "            obs_dict[gsim_no]=df_obs\n",
    "\n",
    "            ### SIMULATION\n",
    "            # get simulated values for station and corresponding period\n",
    "            seg_id = stations.loc[stations['gsim.no']==gsim_no, 'seg_id'].values[0]\n",
    "            idx = ntopo['seg_id'].values.tolist().index(seg_id)\n",
    "\n",
    "            case_mod_dict = {}\n",
    "            metric_station_dict = {}\n",
    "\n",
    "            for case in cases: \n",
    "                mod = ds_sim_dict[case]['discharge'].sel(seg=int(idx))\n",
    "\n",
    "                mod_mean = mod.resample(time=freq).mean(dim='time')\n",
    "                mod_sd   = mod.resample(time=freq).std(dim='time')\n",
    "                mod_min  = mod.resample(time=freq).min(dim='time')\n",
    "                mod_max  = mod.resample(time=freq).max(dim='time')\n",
    "                mod_75   = mod.resample(time=freq).quantile(0.75,dim='time')\n",
    "                mod_25    = mod.resample(time=freq).quantile(0.25,dim='time')\n",
    "                \n",
    "                # assign indices to dataframe with corresponding periods to obs\n",
    "                df_mod = pd.DataFrame(columns=df_obs.columns,index=df_obs.index)\n",
    "                idt = ds_sim_dict[case]['time'][sim_period.isin(period_overlap)]\n",
    "\n",
    "                df_mod['MEAN'] = mod_mean.sel(time=idt)\n",
    "                df_mod['SD']   = mod_sd.sel(time=idt)\n",
    "                df_mod['CV']   = mod_sd.sel(time=idt) / mod_mean.sel(time=idt)\n",
    "                df_mod['MIN']  = mod_min.sel(time=idt)\n",
    "                df_mod['MAX']  = mod_max.sel(time=idt)\n",
    "                df_mod['IQR']  = mod_75.sel(time=idt) - mod_25.sel(time=idt)\n",
    "\n",
    "                case_mod_dict[case] = df_mod\n",
    "\n",
    "                # create dataseries per index\n",
    "                kge = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                pearson = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                alpha = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                beta = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "\n",
    "                nse = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                pbias = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                pabias = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                rmse = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "                mare = pd.Series(np.empty(10),index = df_obs.keys() )\n",
    "\n",
    "                for index in df_obs.keys(): \n",
    "                    temp = df_obs[df_mod.notnull()]\n",
    "                    obs_index = temp[temp[index].notna()][index].values\n",
    "\n",
    "                    temp = df_mod[df_obs.notnull()]\n",
    "                    mod_index = temp[temp[index].notna()][index].values\n",
    "                    \n",
    "                    if (obs_index.size==0 or mod_index.size==0): \n",
    "                        kge[index]   = np.nan\n",
    "                        pearson[index]   = np.nan\n",
    "                        alpha[index]   = np.nan\n",
    "                        beta[index]   = np.nan\n",
    "                        nse[index]   = np.nan\n",
    "                        rmse[index]  = np.nan\n",
    "                        pbias[index] = np.nan\n",
    "                        pabias[index] = np.nan\n",
    "                        mare[index]  = np.nan\n",
    "                    else:    \n",
    "                        kge[index], pearson[index], alpha[index],  beta[index] = he.evaluator(he.kge,obs_index,mod_index)\n",
    "                        nse[index]   = he.evaluator(he.nse,obs_index,mod_index)[0]\n",
    "                        rmse[index]  = he.evaluator(he.rmse,obs_index,mod_index)[0]\n",
    "                        pbias[index] = he.evaluator(he.pbias,obs_index,mod_index)[0]\n",
    "                        pabias[index] = np.sum(abs(mod_index - obs_index)/obs_index)) *100\n",
    "                        \n",
    "                        mare[index]  = he.evaluator(he.mare,obs_index,mod_index)[0]\n",
    "           \n",
    "                \n",
    "                metric_station_dict[case] = pd.DataFrame(data={'NSE':nse,'RMSE':rmse,'PBIAS':pbias,'PABIAS': pabias,'KGE':kge, 'MARE':mare, 'pearson': pearson, 'alpha':alpha, 'beta': beta})\n",
    "\n",
    "            mod_dict[gsim_no] = case_mod_dict\n",
    "            metric_dict[gsim_no] = metric_station_dict\n",
    "    metrics_percase_dict = pd.DataFrame(metric_dict).transpose().to_dict()\n",
    "\n",
    "\n",
    "    # save dicts per stations\n",
    "    import pickle\n",
    "    f = open(datadir+\"/gsim/processed/mod_dict_gsim.pkl\",\"wb\")\n",
    "    pickle.dump(mod_dict,f)\n",
    "    f.close()\n",
    "\n",
    "    f = open(datadir+\"/gsim/processed/metric_dict_gsim.pkl\",\"wb\")\n",
    "    pickle.dump(metric_dict,f)\n",
    "    f.close()\n",
    "\n",
    "else: \n",
    "    mod_dict = pickle.load( open(datadir+\"/gsim/processed/mod_dict_gsim.pkl\", \"rb\" ) )\n",
    "    metric_dict = pickle.load( open( datadir+\"/gsim/processed/metric_dict_gsim.pkl\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter only controlled stations\n",
    "this is now done above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate differences in metrics between cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_differences = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 20.3 ms, total: 24.4 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if calc_differences: \n",
    "\n",
    "    # calculate delta metrics\n",
    "    metrics = ['NSE','KGE','RMSE', 'MARE','PBIAS','PABIAS', 'pearson', 'alpha', 'beta']\n",
    "    metric_dnolak_dict = {}\n",
    "    metric_dnatlak_dict = {}\n",
    "    metric_dnatlak_nolak_dict = {}\n",
    "\n",
    "    # delta\n",
    "\n",
    "    for gsim_no in metric_dict.keys():\n",
    "        metric_dnolak_dict[gsim_no]  = metric_dict[gsim_no][case_H06]-metric_dict[gsim_no][case_nolake]\n",
    "        metric_dnatlak_dict[gsim_no] = metric_dict[gsim_no][case_H06]-metric_dict[gsim_no][case_natlake]\n",
    "        metric_dnatlak_nolak_dict[gsim_no] = metric_dict[gsim_no][case_natlake]-metric_dict[gsim_no][case_nolake]\n",
    "\n",
    "\n",
    "    # create dataframe for certain metric per case and per station\n",
    "    df_metrics_perstation = pd.DataFrame({'gsim_no':list(metric_dict.keys())})\n",
    "    metrics_dnolak_dict = {}\n",
    "    metrics_dnatlak_dict = {}\n",
    "    metrics_dnatlak_nolak_dict = {}\n",
    "    for metric in metrics:\n",
    "        col = pd.DataFrame()\n",
    "        for gsim_no in metric_dict.keys(): \n",
    "            df_nolak = pd.DataFrame(metric_dnolak_dict[gsim_no][metric]).transpose()\n",
    "            df_nolak['gsim_no'] = gsim_no\n",
    "            col=col.append(df_nolak)\n",
    "        metrics_dnolak_dict[metric] = df_metrics_perstation.merge(col,on='gsim_no',how='left')\n",
    "\n",
    "        col_natlak = pd.DataFrame()\n",
    "        for gsim_no in metric_dict.keys(): \n",
    "            df_natlak = pd.DataFrame(metric_dnatlak_dict[gsim_no][metric]).transpose()\n",
    "            df_natlak['gsim_no'] = gsim_no\n",
    "            col_natlak=col_natlak.append(df_natlak)\n",
    "        metrics_dnatlak_dict[metric] = df_metrics_perstation.merge(col_natlak,on='gsim_no',how='left')\n",
    "\n",
    "\n",
    "        col_natlak_nolak = pd.DataFrame()\n",
    "        for gsim_no in metric_dict.keys(): \n",
    "            df_natlak_nolak = pd.DataFrame(metric_dnatlak_nolak_dict[gsim_no][metric]).transpose()\n",
    "            df_natlak_nolak['gsim_no'] = gsim_no\n",
    "            col_natlak_nolak=col_natlak_nolak.append(df_natlak_nolak)\n",
    "        metrics_dnatlak_nolak_dict[metric] = df_metrics_perstation.merge(col_natlak_nolak,on='gsim_no',how='left')\n",
    "        \n",
    "    f = open(datadir+\"/gsim/processed/metrics_dnolak_dict.pkl\",\"wb\")\n",
    "    pickle.dump(metrics_dnolak_dict,f)\n",
    "    f.close()\n",
    "\n",
    "    f = open(datadir+\"/gsim/processed/metrics_dnatlak_dict.pkl\",\"wb\")\n",
    "    pickle.dump(metrics_dnatlak_dict,f)\n",
    "    f.close()\n",
    "\n",
    "else: \n",
    "    \n",
    "    metrics_dnatlak_dict = pickle.load( open(datadir+\"/gsim/processed/metric_dnatlak_dict.pkl\", \"rb\" ) )\n",
    "    metrics_dnolak_dict = pickle.load( open( datadir+\"/gsim/processed/metric_dnolak_dict.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort data in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 0 ns, total: 1.35 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if sort_indices: \n",
    "# sort per index for all stations\n",
    "\n",
    "    indices = ['MEAN', 'SD', 'CV', 'IQR', 'MIN', 'MAX', 'MIN7', 'MAX7']\n",
    "\n",
    "\n",
    "    metrics_perindex_percase_dict = {}\n",
    "    for case in cases: \n",
    "        metrics_perindex_dict = {}\n",
    "        for index in indices:\n",
    "            index_all_stations = []\n",
    "\n",
    "            for station in metrics_percase_dict[case].keys(): \n",
    "                station_values = metrics_percase_dict[case][station].loc[index].values.squeeze()\n",
    "                index_all_stations.append(station_values)\n",
    "            metrics_allstations = pd.DataFrame(np.array(index_all_stations), index=metrics_percase_dict[case], columns= metrics_percase_dict[case][station].keys())\n",
    "            metrics_perindex_dict[index] = metrics_allstations\n",
    "\n",
    "        metrics_perindex_percase_dict[case] = metrics_perindex_dict\n",
    "        \n",
    "    f = open(datadir+\"/gsim/processed/metrics_perindex_dict.pkl\",\"wb\")\n",
    "    pickle.dump(metrics_perindex_dict,f)\n",
    "    f.close()\n",
    "    # sort per metric for all stations\n",
    "    metrics = ['KGE', 'NSE','RMSE', 'MARE','PBIAS', 'PABIAS']\n",
    "\n",
    "    indices_permetric_percase_dict = {}\n",
    "    for case in cases: \n",
    "        indices_permetric_dict = {}\n",
    "        for metric in metrics:\n",
    "            metric_all_stations = []\n",
    "\n",
    "            for station in metrics_percase_dict[case].keys(): \n",
    "                station_values = metrics_percase_dict[case][station][metric].values.squeeze()\n",
    "                metric_all_stations.append(station_values)\n",
    "\n",
    "            indices_allstations = pd.DataFrame(np.array(metric_all_stations), index=metrics_percase_dict[case], columns= metrics_percase_dict[case][station].index.values)\n",
    "            indices_permetric_dict[metric] = indices_allstations\n",
    "\n",
    "        indices_permetric_percase_dict[case] = indices_permetric_dict\n",
    "        \n",
    "    f = open(datadir+\"/gsim/processed/indices_permetric_percase_dict.pkl\",\"wb\")\n",
    "    pickle.dump(indices_permetric_percase_dict,f)\n",
    "    f.close()\n",
    "    \n",
    "else: \n",
    "    metrics_perindex_dict = pickle.load( open(datadir+\"/gsim/processed/metrics_perindex_dict.pkl\", \"rb\" ) )\n",
    "    indices_permetric_percase_dict = pickle.load( open(datadir+\"/gsim/processed/indices_permetric_percase_dict.pkl\", \"rb\" ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-ctsm]",
   "language": "python",
   "name": "conda-env-miniconda-ctsm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
